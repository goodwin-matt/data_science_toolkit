What is the first step of PCA?	Standardize variables by subtracting the mean and dividing by the standard deviation. We do this because the scale differences can throw the transformations off (think about how we are finding the direction of greatest variability in the dataset).
What is the second step of PCA?	Find the covariance matrix of newly standardized data matrix. Rember this is [latex]$1/n X'_c X_c$[/latex]
What is the third step of PCA, after standardizing and finding covariance matrix?	Find the eigenvalue decomposition of the covariance matrix
What is the fourth step of PCA, after standardizing, finding covariance, and diagonalizing?	Multiply the standardized data matrix by the eigenvectors
How/why are eigenvalues and eigenvectors involved in PCA?	This provides a method of diagonalizing the covariance matrix. By transforming the data matrix using these eigenvectors we then will have a covariance matrix with 0's on the diagonal and variance given by the eigenvalues, implying ortogonal variables.
What is a principal component?	<div>Principal components are uncorrelated variables that are the result of an orthogonal transformation on the original variables. You can think of that orthogonal transformation as projecting each data point onto the vectors that are in the direction of greatest variability in the data. So for example, the first principal component will be the variable that goes along in the direction of greatest variability in the dataset.</div>
Why do we use principal components?	<div><div>We primarily use it as a dimentionality reduction technique by taking the top principal components that describe a sufficient amount of the variability in the data. Essentially we are finding new variables that are a combination of the old variables to better describe the data and reduce the redundancy in variables or retain the most valuable parts of all the features.&nbsp;</div></div>
What is the covariance of the data matrix?	1/nX_c'X_c (the centered data matrix). The data matrix is going to be nxp (n instances, p features). The covariance matrix will be pxp (the Sigma_ij position will be the covariance between the ith variable and the jth variable). We use the n instances however to calculate the SAMPLE covariance.
How does k-means formula work?	1. Decide on <i>k</i> (hyper-parameter), the number of clusters.&nbsp;<div>2. Choose <i>k</i> random datapoints.&nbsp;</div><div>3. Calculate the difference between each data point and each chosen <i>k </i>point. Assign each data point to closest <i>k</i>&nbsp;point.&nbsp;</div><div>4. Calculate centroid - either average or median of each cluster</div><div>5. Recalculate the distance of each point to each centroid and assign to the closest one.</div><div>6. Continue this process of recalculating centroid and reassigning until there are no more changes in assignment. Or when the centroid doesn't change anymore.</div>
What is the first step of the t-sne algorithm?	Find probabilities p_ij that are proportional to the similarity of object x_i and x_j in the high dimensional space. This is done by first finding p_j|i which is taken as the similarity of x_j to x_i and is the probability that x_i would pick x_j as its neighbor if neighbors were picked in proportion to their Guassian probability distribution centered over x_i. p_ij is then calculated as (p_j|i + p_i|j)/2N
What is the second step of tsne?	"Find a map [latex]$y_{i,..n}$[/latex] in dimension <i>d</i>&nbsp;that reflects the similarities [latex]$p_{ij}$[/latex] as well as possible. The way it does this is to calculate similar metrics [latex]$q_{ij}$[/latex] in the lower dimensions using a t-distribution since the t has fatter tails and will encode disimilar points further away from each other better. The way the points <i><b>y</b></i> are found is to minimize the Kullback-Leibler divergence between the two distributions (using gradient descent)."
What is the silloutee metric?	This takes the average intra-cluster distance (a) and the smallest average distance from each sample in the cluster to another cluster (b) and calculates (b-a)/max(a,b). 1 is a perfect score and -1 is the worse
What is hierarchical clustering?	Take two closest samples and merge and keep merging the nearest sample until all are in one cluster. Then view the dendrogram and make a decision on where to cut to create clusters.
What is/was a good reason to use PCA?	1. Correlated variables changed to orthogonal, handy when doing regression.<div>2. In todays world, a lot of models can now handle the dimensionalty problems that come with many features. PCA was/is more valuable when dealing with regressions, etc.</div>
What does it mean to have a linear dimensionality reduction technique vs. a nonlinear one?	"If f1,f2 are linear then it is a linear reduction technique. PCA is linear because f1 is a projection onto the first eigenvector, f2 onto the second eigenvector, etc.<div><img src=""15 0 0.png"">&nbsp;</div>"
What is the aim of dimensionality reduction according to the t-sne paper?	"<div>To preserve as much of the significant structure of the high-dimensional data as possible in the low-dimensional map.</div><div><br></div><div><span style=""text-align: left;"">For high-dimensional data that lies on or near a low-dimensional, non-linear manifold it is usually more important to keep the low-dimensional representations of very similar datapoints close together, which is typically not possible with a linear mapping. A manifold is simply a space that resembles a Euclidean space in some dimension.</span><br></div><div><span style=""text-align: left;""><br></span></div><div><span style=""text-align: left;"">T-SNE's goal is to keep similar points close together whereas PCA is a linear transformation.</span></div><div><br></div><div>PCA - ""<span style=""text-align: justify;"">It focuses on preserving the distances between widely separated data points rather than on preserving the distances between nearby data points."" This is because the points that are spread out will find themselves on the eigenvector of greatest variability and when it is projected on to that vector it won't be much different.&nbsp;</span></div><div><span style=""color: rgb(89, 88, 88); font-family: roboto, sans-serif; font-size: 15px; text-align: justify;""><br></span></div><div><img src=""16 1 0.png""><span style=""color: rgb(89, 88, 88); font-family: roboto, sans-serif; font-size: 15px; text-align: justify;""><br></span></div> "

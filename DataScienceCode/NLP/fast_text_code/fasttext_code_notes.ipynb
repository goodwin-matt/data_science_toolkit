{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2vec code notes using FastText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General notes on how to use fastText using the python wrapper(?) found here: https://github.com/facebookresearch/fastText/tree/master/python. Command line implementation can also be used found here: https://fasttext.cc/docs/en/support.html. This version also creates a .vec file which is needed to view in tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastText as ft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ft.FastText.train_unsupervised('test.txt', model='skipgram')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get words used in model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['</s>',\n",
       " 'the',\n",
       " 'is',\n",
       " 'of',\n",
       " 'a',\n",
       " '{\\\\partial',\n",
       " 'Jacobian',\n",
       " '{\\\\displaystyle',\n",
       " '\\\\varphi',\n",
       " 'function',\n",
       " '\\\\mathbf',\n",
       " 'and',\n",
       " '{f}',\n",
       " 'to',\n",
       " '\\\\theta',\n",
       " '{J}',\n",
       " 'in',\n",
       " 'matrix',\n",
       " '_{\\\\mathbf',\n",
       " 'determinant',\n",
       " 'f',\n",
       " 'point',\n",
       " 'The',\n",
       " '{x}',\n",
       " 'that',\n",
       " 'at',\n",
       " 'if',\n",
       " '{p}',\n",
       " 'as',\n",
       " 'near',\n",
       " '=',\n",
       " 'then',\n",
       " 'by',\n",
       " 'with',\n",
       " '→',\n",
       " '2',\n",
       " 'Example',\n",
       " 'x}{\\\\partial',\n",
       " 'y}{\\\\partial',\n",
       " 'r}}&{\\\\dfrac',\n",
       " 'an',\n",
       " 'its',\n",
       " 'for',\n",
       " 'this',\n",
       " 'be',\n",
       " 'p',\n",
       " 'ℝn',\n",
       " 'f_{1}}{\\\\partial',\n",
       " 'This',\n",
       " 'If',\n",
       " '{F}',\n",
       " '\\\\sin',\n",
       " 'or',\n",
       " '}(\\\\mathbf',\n",
       " 'derivative',\n",
       " 'x_{1}}}&{\\\\dfrac',\n",
       " 'linear',\n",
       " '{R}',\n",
       " '\\\\mathbb',\n",
       " '\\\\cos',\n",
       " 'x_{2}}}&{\\\\dfrac',\n",
       " 'In',\n",
       " 'are',\n",
       " 'given',\n",
       " 'differentiable',\n",
       " 'not',\n",
       " 'coordinates',\n",
       " 'transformation',\n",
       " 'x',\n",
       " '1',\n",
       " 'inverse',\n",
       " 'z}{\\\\partial',\n",
       " 'which',\n",
       " '}}&{\\\\dfrac',\n",
       " 'y_{1}}{\\\\partial',\n",
       " ':',\n",
       " '×',\n",
       " 'can',\n",
       " '&r\\\\cos',\n",
       " 'x_{3}}}\\\\\\\\[1em]{\\\\dfrac',\n",
       " 'y_{2}}{\\\\partial',\n",
       " '{\\\\mathbf',\n",
       " 'square',\n",
       " 'also',\n",
       " 'y_{3}}{\\\\partial',\n",
       " '&-r\\\\sin',\n",
       " 'y_{4}}{\\\\partial',\n",
       " 'where',\n",
       " 'invertible',\n",
       " '&{\\\\dfrac',\n",
       " 'from',\n",
       " 'vector',\n",
       " 'x_{1}}}&\\\\cdots',\n",
       " '}}\\\\\\\\[1em]{\\\\dfrac',\n",
       " 'gradient',\n",
       " 'scalar',\n",
       " 'when',\n",
       " 'complex',\n",
       " 'system',\n",
       " 'F',\n",
       " 'matrix,',\n",
       " ')}',\n",
       " 'original']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_words()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get vector representation of word - doesn't have to be in the original model because of FastTexts use of subwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-8.9638030e-05, -7.0428365e-04, -1.1684634e-03, -1.4596151e-03,\n",
       "       -1.2494313e-03,  2.4219018e-03, -2.0084984e-03, -3.2501675e-03,\n",
       "        1.6478845e-05,  1.4669150e-03, -2.8544320e-03, -2.0823495e-03,\n",
       "       -5.5171753e-04, -2.6905767e-03, -1.4973406e-03,  2.5300994e-03,\n",
       "        5.9868972e-04,  4.9233111e-04, -3.6145106e-03, -1.6231959e-03,\n",
       "        1.0269432e-03,  1.6680023e-03,  7.2455866e-04, -1.2140825e-03,\n",
       "        1.2267560e-03, -1.4175571e-03,  8.7878871e-04, -7.0040917e-04,\n",
       "       -2.4098430e-03, -8.7106618e-04,  3.1986047e-04,  2.9065218e-04,\n",
       "        7.3973241e-04, -1.8543201e-04, -3.6539481e-04, -8.5424253e-04,\n",
       "        2.8047115e-03, -1.6290577e-03, -1.4359256e-03,  1.1007186e-03,\n",
       "        4.5971574e-05,  1.7826334e-03, -3.4032650e-03, -5.1183969e-04,\n",
       "       -2.5152395e-04,  1.5845661e-03,  1.7632304e-03,  4.3029650e-04,\n",
       "        4.5361611e-04,  2.9376587e-03, -2.4226611e-03,  5.3946197e-04,\n",
       "       -3.1688255e-03,  2.1293397e-04,  2.5133782e-03,  4.4310442e-04,\n",
       "       -2.6244444e-03, -3.0333938e-03,  5.0817215e-04,  1.7640872e-03,\n",
       "       -3.3391849e-03,  9.3018200e-04, -4.0571924e-04,  5.7407602e-04,\n",
       "       -3.8689903e-03,  2.3083072e-03,  1.5137736e-03,  2.4787576e-03,\n",
       "        2.9058547e-03,  3.6016709e-04,  2.9480865e-03,  3.3194460e-03,\n",
       "        1.6859412e-03,  3.2110200e-03, -4.6821823e-04, -3.8462894e-03,\n",
       "       -1.9075604e-04, -7.1001021e-05, -2.7943498e-03,  1.5113036e-03,\n",
       "        1.3342118e-03, -3.8416786e-04, -8.0419902e-04, -1.4933801e-03,\n",
       "       -1.8652737e-03,  2.1347858e-03,  1.6593999e-04,  2.2133936e-03,\n",
       "        8.4652379e-04, -7.3639245e-04, -6.7549309e-04,  8.8725641e-04,\n",
       "        1.1300727e-03, -2.5064787e-03, -1.3304349e-03,  3.3280083e-03,\n",
       "       -8.4172230e-04, -2.0619156e-05,  2.9869718e-03, -2.4087147e-03],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_word_vector('king')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_model(\"test_model.bin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ft.FastText.load_model('test_model.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Should be same as above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-8.9638030e-05, -7.0428365e-04, -1.1684634e-03, -1.4596151e-03,\n",
       "       -1.2494313e-03,  2.4219018e-03, -2.0084984e-03, -3.2501675e-03,\n",
       "        1.6478845e-05,  1.4669150e-03, -2.8544320e-03, -2.0823495e-03,\n",
       "       -5.5171753e-04, -2.6905767e-03, -1.4973406e-03,  2.5300994e-03,\n",
       "        5.9868972e-04,  4.9233111e-04, -3.6145106e-03, -1.6231959e-03,\n",
       "        1.0269432e-03,  1.6680023e-03,  7.2455866e-04, -1.2140825e-03,\n",
       "        1.2267560e-03, -1.4175571e-03,  8.7878871e-04, -7.0040917e-04,\n",
       "       -2.4098430e-03, -8.7106618e-04,  3.1986047e-04,  2.9065218e-04,\n",
       "        7.3973241e-04, -1.8543201e-04, -3.6539481e-04, -8.5424253e-04,\n",
       "        2.8047115e-03, -1.6290577e-03, -1.4359256e-03,  1.1007186e-03,\n",
       "        4.5971574e-05,  1.7826334e-03, -3.4032650e-03, -5.1183969e-04,\n",
       "       -2.5152395e-04,  1.5845661e-03,  1.7632304e-03,  4.3029650e-04,\n",
       "        4.5361611e-04,  2.9376587e-03, -2.4226611e-03,  5.3946197e-04,\n",
       "       -3.1688255e-03,  2.1293397e-04,  2.5133782e-03,  4.4310442e-04,\n",
       "       -2.6244444e-03, -3.0333938e-03,  5.0817215e-04,  1.7640872e-03,\n",
       "       -3.3391849e-03,  9.3018200e-04, -4.0571924e-04,  5.7407602e-04,\n",
       "       -3.8689903e-03,  2.3083072e-03,  1.5137736e-03,  2.4787576e-03,\n",
       "        2.9058547e-03,  3.6016709e-04,  2.9480865e-03,  3.3194460e-03,\n",
       "        1.6859412e-03,  3.2110200e-03, -4.6821823e-04, -3.8462894e-03,\n",
       "       -1.9075604e-04, -7.1001021e-05, -2.7943498e-03,  1.5113036e-03,\n",
       "        1.3342118e-03, -3.8416786e-04, -8.0419902e-04, -1.4933801e-03,\n",
       "       -1.8652737e-03,  2.1347858e-03,  1.6593999e-04,  2.2133936e-03,\n",
       "        8.4652379e-04, -7.3639245e-04, -6.7549309e-04,  8.8725641e-04,\n",
       "        1.1300727e-03, -2.5064787e-03, -1.3304349e-03,  3.3280083e-03,\n",
       "       -8.4172230e-04, -2.0619156e-05,  2.9869718e-03, -2.4087147e-03],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_word_vector('king')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected dimensions: 11  X  100\n",
      "(11, 100)\n",
      "z =  109\n",
      "Num nodes: 11\n",
      "Run `tensorboard --logdir=projections --port 8088 --host 0.0.0.0` to run visualize result on tensorboard\n"
     ]
    }
   ],
   "source": [
    "# From https://gist.github.com/nlothian/0cd4540389f7091717ece6f4b89b6604\n",
    "\n",
    "meta_file = \"g2x_metadata.tsv\"\n",
    "output_path = \"projections\"\n",
    "vec_file = 'model.vec'\n",
    "\n",
    "# read embedding file into list and get the size\n",
    "with open(vec_file, 'r') as embedding_file:\n",
    "    embedding_content = embedding_file.readlines()\n",
    "    embedding_content = [x.strip() for x in embedding_content] \n",
    "\n",
    "\n",
    "    num_lines = len(embedding_content) - 1 # skip the header\n",
    "    num_dims = len(embedding_content[1].split()) - 1 # -1 because of the label column\n",
    "    print(\"Detected dimensions:\", num_lines, \" X \", num_dims)\n",
    "\n",
    "    placeholder = np.zeros((num_lines, num_dims))\n",
    "\n",
    "    print(placeholder.shape)\n",
    "\n",
    "\n",
    "    z = 0\n",
    "    with open(os.path.join(output_path, meta_file), 'w') as file_metadata:\n",
    "\n",
    "        i = 0\n",
    "        for line in embedding_content[1:]:  # skip the header line\n",
    "            values = line.split()\n",
    "            raw_label = values[0]\n",
    "            #print(label)\n",
    "            col = 0\n",
    "            for val in values[1:]: # skip the label\n",
    "                placeholder[i][col] = val\n",
    "                z = i + col\n",
    "                col = col + 1\n",
    "            i = i + 1\n",
    "\n",
    "            if raw_label == '':\n",
    "                file_metadata.write(\"<Empty Line>\\n\")\n",
    "            else:\n",
    "                label = raw_label\n",
    "                file_metadata.write(label + \"\\n\")\n",
    "\n",
    "        print(\"z = \", z)\n",
    "\n",
    "    # define the model without training\n",
    "    sess = tf.InteractiveSession()\n",
    "\n",
    "    embedding = tf.Variable(placeholder, trainable=False, name='g2x_metadata')\n",
    "    tf.global_variables_initializer().run()\n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "    writer = tf.summary.FileWriter(output_path, sess.graph)\n",
    "\n",
    "    # adding into projector\n",
    "    config = projector.ProjectorConfig()\n",
    "    embed = config.embeddings.add()\n",
    "    embed.tensor_name = 'g2x_metadata'\n",
    "    embed.metadata_path = meta_file\n",
    "\n",
    "    # Specify the width and height of a single thumbnail.\n",
    "    projector.visualize_embeddings(writer, config)\n",
    "    saver.save(sess, os.path.join(output_path, 'g2x_metadata.ckpt'))\n",
    "    print('Num nodes: {}'.format(num_lines))\n",
    "    print('Run `tensorboard --logdir={0} --port 8088 --host 0.0.0.0` to run visualize result on tensorboard'.format(output_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
